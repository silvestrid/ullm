{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ullm - Lightweight LLM Interface","text":"<p>ullm (\u03bcLLM) is a lightweight, fast alternative to litellm designed specifically for applications that need a minimal, efficient LLM interface.</p>"},{"location":"#why-ullm","title":"Why ullm?","text":""},{"location":"#performance-first","title":"Performance First","text":"<ul> <li>100x smaller memory footprint: ~2MB vs ~200MB for litellm</li> <li>24x faster import time: ~50ms vs ~1.2s</li> <li>Minimal dependencies: Only 3 core dependencies</li> </ul>"},{"location":"#production-ready","title":"Production Ready","text":"<ul> <li>\u2705 Full litellm API compatibility</li> <li>\u2705 Async/await support throughout</li> <li>\u2705 Streaming (sync and async)</li> <li>\u2705 Tool calling / function calling</li> <li>\u2705 Structured output with Pydantic</li> <li>\u2705 Exponential backoff retry logic</li> <li>\u2705 Comprehensive test coverage</li> </ul>"},{"location":"#supported-providers","title":"Supported Providers","text":"Provider Models Status OpenAI GPT-4, GPT-3.5, o1, o3, etc. \u2705 Full support Anthropic Claude 3 (Opus, Sonnet, Haiku) \u2705 Full support Groq Llama 3.1, Mixtral, etc. \u2705 Full support AWS Bedrock Claude via Bedrock \u2705 Full support"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import ullm\n\n# Simple completion\nresponse = ullm.completion(\n    model=\"openai/gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response.choices[0].message.content)\n\n# Streaming\nfor chunk in ullm.completion(\n    model=\"anthropic/claude-3-5-sonnet-20241022\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n):\n    print(chunk.choices[0].delta.content, end=\"\")\n\n# Async\nimport asyncio\n\nasync def main():\n    response = await ullm.acompletion(\n        model=\"groq/llama-3.1-8b-instant\",\n        messages=[{\"role\": \"user\", \"content\": \"Hi!\"}]\n    )\n    print(response.choices[0].message.content)\n\nasyncio.run(main())\n</code></pre>"},{"location":"#design-philosophy","title":"Design Philosophy","text":"<ol> <li>Lightweight First: Every line of code and dependency must justify its existence</li> <li>Compatibility Second: Stay compatible with litellm where practical, but not at cost of bloat</li> <li>Modern Tooling: Use the best tools (uv, ruff, httpx)</li> <li>Pragmatic Over Perfect: Ship working code, iterate based on feedback</li> <li>Clear Over Clever: Readable code beats clever abstractions</li> </ol>"},{"location":"#comparison-with-litellm","title":"Comparison with litellm","text":"Feature litellm ullm Notes Memory footprint ~200MB ~2MB 100x smaller Import time ~1.2s ~50ms 24x faster Dependencies 50+ 3 Minimal overhead Providers 100+ 4 Core providers only completion() \u2705 \u2705 Full compatibility streaming \u2705 \u2705 Sync and async tool calling \u2705 \u2705 OpenAI format structured output \u2705 \u2705 Pydantic support caching \u2705 \u274c By design (DSPy has it)"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p>:material-clock-fast:{ .lg .middle } Quick Start</p> <p>Get up and running in 5 minutes</p> <p>:octicons-arrow-right-24: Quick Start</p> </li> <li> <p>:material-book-open-variant:{ .lg .middle } User Guide</p> <p>Learn about all features</p> <p>:octicons-arrow-right-24: User Guide</p> </li> <li> <p>:material-api:{ .lg .middle } API Reference</p> <p>Detailed API documentation</p> <p>:octicons-arrow-right-24: API Reference</p> </li> <li> <p>:material-code-braces:{ .lg .middle } Contributing</p> <p>Help improve ullm</p> <p>:octicons-arrow-right-24: Contributing</p> </li> </ul>"},{"location":"#license","title":"License","text":"<p>ullm is released under the MIT License.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#010-2025-01-xx","title":"0.1.0 - 2025-01-XX","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Initial release of ullm</li> <li>Support for OpenAI, Anthropic, Groq, and AWS Bedrock providers</li> <li>Chat completion API (<code>completion()</code> and <code>acompletion()</code>)</li> <li>OpenAI Responses API (<code>responses()</code> and <code>aresponses()</code>)</li> <li>Streaming support for all providers</li> <li>Tool calling/function calling support</li> <li>Structured output with Pydantic models</li> <li>Exponential backoff retry logic using tenacity</li> <li>Comprehensive test suite</li> <li>CI/CD with GitHub Actions</li> <li>Full API compatibility with litellm core features</li> <li>Examples and documentation</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>100x smaller memory footprint than litellm (~2MB vs ~200MB)</li> <li>24x faster import time</li> <li>Modern Python tooling (uv, ruff, mypy)</li> <li>Type hints throughout</li> <li>Async/await support</li> <li>Minimal dependencies (httpx, pydantic, tenacity)</li> </ul>"},{"location":"api/completion/","title":"Completion API","text":"<p>(Full API reference coming soon)</p>"},{"location":"api/completion/#ullm.completion","title":"<code>completion(model, messages, temperature=None, max_tokens=None, stream=False, tools=None, tool_choice=None, response_format=None, num_retries=3, retry_strategy='exponential_backoff_retry', cache=None, api_key=None, api_base=None, timeout=600.0, **kwargs)</code>","text":"<p>Make a completion request to an LLM provider.</p> <p>Compatible with litellm.completion() API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name in format \"provider/model-name\" or just \"model-name\"</p> required <code>messages</code> <code>list[Dict[str, Any]]</code> <p>List of message dicts with \"role\" and \"content\"</p> required <code>temperature</code> <code>Optional[float]</code> <p>Sampling temperature (0-2)</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum tokens to generate</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the response</p> <code>False</code> <code>tools</code> <code>Optional[list[Tool]]</code> <p>List of tool/function definitions</p> <code>None</code> <code>tool_choice</code> <code>Optional[Union[str, Dict[str, Any]]]</code> <p>How to choose tools (\"auto\", \"required\", or specific tool)</p> <code>None</code> <code>response_format</code> <code>Optional[ResponseFormat]</code> <p>Response format (dict or Pydantic model)</p> <code>None</code> <code>num_retries</code> <code>int</code> <p>Number of retries on rate limit or timeout</p> <code>3</code> <code>retry_strategy</code> <code>str</code> <p>Retry strategy (currently only \"exponential_backoff_retry\")</p> <code>'exponential_backoff_retry'</code> <code>cache</code> <code>Optional[Dict[str, Any]]</code> <p>Cache control dict (for compatibility, not used by ullm)</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>API key (if not in environment)</p> <code>None</code> <code>api_base</code> <code>Optional[str]</code> <p>API base URL (if not default)</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Request timeout in seconds</p> <code>600.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[ModelResponse, Iterator[StreamChunk]]</code> <p>ModelResponse or Iterator[StreamChunk] if streaming</p> <p>Raises:</p> Type Description <code>AuthenticationError</code> <p>On authentication failure</p> <code>BadRequestError</code> <p>On invalid request</p> <code>RateLimitError</code> <p>On rate limit exceeded</p> <code>Timeout</code> <p>On request timeout</p> <code>APIError</code> <p>On other API errors</p> Source code in <code>ullm/main.py</code> <pre><code>def completion(\n    model: str,\n    messages: list[Dict[str, Any]],\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stream: bool = False,\n    tools: Optional[list[Tool]] = None,\n    tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n    response_format: Optional[ResponseFormat] = None,\n    num_retries: int = 3,\n    retry_strategy: str = \"exponential_backoff_retry\",\n    cache: Optional[Dict[str, Any]] = None,\n    api_key: Optional[str] = None,\n    api_base: Optional[str] = None,\n    timeout: float = 600.0,\n    **kwargs: Any,\n) -&gt; Union[ModelResponse, Iterator[StreamChunk]]:\n    \"\"\"\n    Make a completion request to an LLM provider.\n\n    Compatible with litellm.completion() API.\n\n    Args:\n        model: Model name in format \"provider/model-name\" or just \"model-name\"\n        messages: List of message dicts with \"role\" and \"content\"\n        temperature: Sampling temperature (0-2)\n        max_tokens: Maximum tokens to generate\n        stream: Whether to stream the response\n        tools: List of tool/function definitions\n        tool_choice: How to choose tools (\"auto\", \"required\", or specific tool)\n        response_format: Response format (dict or Pydantic model)\n        num_retries: Number of retries on rate limit or timeout\n        retry_strategy: Retry strategy (currently only \"exponential_backoff_retry\")\n        cache: Cache control dict (for compatibility, not used by ullm)\n        api_key: API key (if not in environment)\n        api_base: API base URL (if not default)\n        timeout: Request timeout in seconds\n        **kwargs: Additional provider-specific parameters\n\n    Returns:\n        ModelResponse or Iterator[StreamChunk] if streaming\n\n    Raises:\n        AuthenticationError: On authentication failure\n        BadRequestError: On invalid request\n        RateLimitError: On rate limit exceeded\n        Timeout: On request timeout\n        APIError: On other API errors\n    \"\"\"\n    provider, model_name = parse_model_name(model)\n\n    client = _get_client(\n        provider,\n        api_key=api_key,\n        api_base=api_base,\n        timeout=timeout,\n        **kwargs,\n    )\n\n    # Create retry-wrapped function if retries are enabled\n    if num_retries &gt; 0 and not stream:  # Don't retry streaming requests\n\n        @_create_retry_decorator(num_retries)\n        def _make_request():\n            return client.completion(\n                model=model_name,\n                messages=messages,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                stream=stream,\n                tools=tools,\n                tool_choice=tool_choice,\n                response_format=response_format,\n                **kwargs,\n            )\n\n        return _make_request()\n    else:\n        return client.completion(\n            model=model_name,\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            stream=stream,\n            tools=tools,\n            tool_choice=tool_choice,\n            response_format=response_format,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/completion/#ullm.acompletion","title":"<code>acompletion(model, messages, temperature=None, max_tokens=None, stream=False, tools=None, tool_choice=None, response_format=None, num_retries=3, retry_strategy='exponential_backoff_retry', cache=None, api_key=None, api_base=None, timeout=600.0, **kwargs)</code>  <code>async</code>","text":"<p>Make an async completion request to an LLM provider.</p> <p>Compatible with litellm.acompletion() API.</p> <p>Returns:</p> Type Description <code>Union[ModelResponse, AsyncIterator[StreamChunk]]</code> <p>ModelResponse or AsyncIterator[StreamChunk] if streaming</p> Source code in <code>ullm/main.py</code> <pre><code>async def acompletion(\n    model: str,\n    messages: list[Dict[str, Any]],\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stream: bool = False,\n    tools: Optional[list[Tool]] = None,\n    tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n    response_format: Optional[ResponseFormat] = None,\n    num_retries: int = 3,\n    retry_strategy: str = \"exponential_backoff_retry\",\n    cache: Optional[Dict[str, Any]] = None,\n    api_key: Optional[str] = None,\n    api_base: Optional[str] = None,\n    timeout: float = 600.0,\n    **kwargs: Any,\n) -&gt; Union[ModelResponse, AsyncIterator[StreamChunk]]:\n    \"\"\"\n    Make an async completion request to an LLM provider.\n\n    Compatible with litellm.acompletion() API.\n\n    Args:\n        Same as completion()\n\n    Returns:\n        ModelResponse or AsyncIterator[StreamChunk] if streaming\n    \"\"\"\n    provider, model_name = parse_model_name(model)\n\n    client = _get_client(\n        provider,\n        api_key=api_key,\n        api_base=api_base,\n        timeout=timeout,\n        **kwargs,\n    )\n\n    # Create retry-wrapped function if retries are enabled\n    if num_retries &gt; 0 and not stream:\n\n        @_create_retry_decorator(num_retries)\n        async def _make_request():\n            return await client.acompletion(\n                model=model_name,\n                messages=messages,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                stream=stream,\n                tools=tools,\n                tool_choice=tool_choice,\n                response_format=response_format,\n                **kwargs,\n            )\n\n        return await _make_request()\n    else:\n        return await client.acompletion(\n            model=model_name,\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            stream=stream,\n            tools=tools,\n            tool_choice=tool_choice,\n            response_format=response_format,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":"<p>(Full API reference coming soon)</p>"},{"location":"api/exceptions/#ullm.exceptions","title":"<code>exceptions</code>","text":"<p>Exception types for ullm - compatible with litellm exceptions.</p>"},{"location":"api/exceptions/#ullm.exceptions.UllmException","title":"<code>UllmException(message, status_code=None, model=None, llm_provider=None)</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all ullm errors.</p> Source code in <code>ullm/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    status_code: Optional[int] = None,\n    model: Optional[str] = None,\n    llm_provider: Optional[str] = None,\n):\n    super().__init__(message)\n    self.message = message\n    self.status_code = status_code\n    self.model = model\n    self.llm_provider = llm_provider\n</code></pre>"},{"location":"api/exceptions/#ullm.exceptions.AuthenticationError","title":"<code>AuthenticationError(message, model=None, llm_provider=None)</code>","text":"<p>               Bases: <code>UllmException</code></p> <p>Raised when authentication fails (401).</p> Source code in <code>ullm/exceptions.py</code> <pre><code>def __init__(self, message: str, model: Optional[str] = None, llm_provider: Optional[str] = None):\n    super().__init__(message, status_code=401, model=model, llm_provider=llm_provider)\n</code></pre>"},{"location":"api/exceptions/#ullm.exceptions.BadRequestError","title":"<code>BadRequestError(message, model=None, llm_provider=None)</code>","text":"<p>               Bases: <code>UllmException</code></p> <p>Raised for invalid requests (400).</p> Source code in <code>ullm/exceptions.py</code> <pre><code>def __init__(self, message: str, model: Optional[str] = None, llm_provider: Optional[str] = None):\n    super().__init__(message, status_code=400, model=model, llm_provider=llm_provider)\n</code></pre>"},{"location":"api/exceptions/#ullm.exceptions.RateLimitError","title":"<code>RateLimitError(message, model=None, llm_provider=None)</code>","text":"<p>               Bases: <code>UllmException</code></p> <p>Raised when rate limit is exceeded (429).</p> Source code in <code>ullm/exceptions.py</code> <pre><code>def __init__(self, message: str, model: Optional[str] = None, llm_provider: Optional[str] = None):\n    super().__init__(message, status_code=429, model=model, llm_provider=llm_provider)\n</code></pre>"},{"location":"api/exceptions/#ullm.exceptions.Timeout","title":"<code>Timeout(message, model=None, llm_provider=None)</code>","text":"<p>               Bases: <code>UllmException</code></p> <p>Raised on request timeout (504).</p> Source code in <code>ullm/exceptions.py</code> <pre><code>def __init__(self, message: str, model: Optional[str] = None, llm_provider: Optional[str] = None):\n    super().__init__(message, status_code=504, model=model, llm_provider=llm_provider)\n</code></pre>"},{"location":"api/exceptions/#ullm.exceptions.APIError","title":"<code>APIError(message, status_code=500, model=None, llm_provider=None)</code>","text":"<p>               Bases: <code>UllmException</code></p> <p>Raised for general API errors (500+).</p> Source code in <code>ullm/exceptions.py</code> <pre><code>def __init__(\n    self, message: str, status_code: int = 500, model: Optional[str] = None, llm_provider: Optional[str] = None\n):\n    super().__init__(message, status_code=status_code, model=model, llm_provider=llm_provider)\n</code></pre>"},{"location":"api/exceptions/#ullm.exceptions.ProviderNotFoundError","title":"<code>ProviderNotFoundError(model)</code>","text":"<p>               Bases: <code>UllmException</code></p> <p>Raised when provider cannot be determined from model string.</p> Source code in <code>ullm/exceptions.py</code> <pre><code>def __init__(self, model: str):\n    super().__init__(f\"Could not determine provider from model: {model}\", model=model)\n</code></pre>"},{"location":"api/exceptions/#ullm.exceptions.UnsupportedProviderError","title":"<code>UnsupportedProviderError(provider, model)</code>","text":"<p>               Bases: <code>UllmException</code></p> <p>Raised when provider is not supported.</p> Source code in <code>ullm/exceptions.py</code> <pre><code>def __init__(self, provider: str, model: str):\n    super().__init__(f\"Provider '{provider}' is not supported\", model=model, llm_provider=provider)\n</code></pre>"},{"location":"api/responses/","title":"Responses API","text":"<p>(Full API reference coming soon)</p>"},{"location":"api/responses/#ullm.responses","title":"<code>responses(model, input, temperature=None, max_tokens=None, num_retries=3, retry_strategy='exponential_backoff_retry', cache=None, api_key=None, api_base=None, timeout=600.0, **kwargs)</code>","text":"<p>Make a request using OpenAI's Responses API format.</p> <p>Compatible with litellm.responses() API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name in format \"provider/model-name\"</p> required <code>input</code> <code>list[Dict[str, Any]]</code> <p>List of input messages in Responses API format</p> required <code>temperature</code> <code>Optional[float]</code> <p>Sampling temperature</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum tokens to generate</p> <code>None</code> <code>num_retries</code> <code>int</code> <p>Number of retries</p> <code>3</code> <code>retry_strategy</code> <code>str</code> <p>Retry strategy</p> <code>'exponential_backoff_retry'</code> <code>cache</code> <code>Optional[Dict[str, Any]]</code> <p>Cache control (not used)</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>API key</p> <code>None</code> <code>api_base</code> <code>Optional[str]</code> <p>API base URL</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Request timeout</p> <code>600.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>ModelResponse</code> <p>ModelResponse</p> Source code in <code>ullm/main.py</code> <pre><code>def responses(\n    model: str,\n    input: list[Dict[str, Any]],\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    num_retries: int = 3,\n    retry_strategy: str = \"exponential_backoff_retry\",\n    cache: Optional[Dict[str, Any]] = None,\n    api_key: Optional[str] = None,\n    api_base: Optional[str] = None,\n    timeout: float = 600.0,\n    **kwargs: Any,\n) -&gt; ModelResponse:\n    \"\"\"\n    Make a request using OpenAI's Responses API format.\n\n    Compatible with litellm.responses() API.\n\n    Args:\n        model: Model name in format \"provider/model-name\"\n        input: List of input messages in Responses API format\n        temperature: Sampling temperature\n        max_tokens: Maximum tokens to generate\n        num_retries: Number of retries\n        retry_strategy: Retry strategy\n        cache: Cache control (not used)\n        api_key: API key\n        api_base: API base URL\n        timeout: Request timeout\n        **kwargs: Additional parameters\n\n    Returns:\n        ModelResponse\n    \"\"\"\n    provider, model_name = parse_model_name(model)\n\n    client = _get_client(\n        provider,\n        api_key=api_key,\n        api_base=api_base,\n        timeout=timeout,\n        **kwargs,\n    )\n\n    # Only OpenAI client has responses method\n    if not hasattr(client, \"responses\"):\n        # Fall back to converting to messages format\n        messages = []\n        for item in input:\n            role = item.get(\"role\", \"user\")\n            content_blocks = item.get(\"content\", [])\n\n            if isinstance(content_blocks, list):\n                content = \" \".join(\n                    block.get(\"text\", \"\") if isinstance(block, dict) else str(block) for block in content_blocks\n                )\n            else:\n                content = str(content_blocks)\n\n            messages.append({\"role\": role, \"content\": content})\n\n        return completion(\n            model=model,\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            num_retries=num_retries,\n            api_key=api_key,\n            api_base=api_base,\n            timeout=timeout,\n            **kwargs,\n        )\n\n    # Use native responses method\n    if num_retries &gt; 0:\n\n        @_create_retry_decorator(num_retries)\n        def _make_request():\n            return client.responses(\n                model=model_name, input=input, temperature=temperature, max_tokens=max_tokens, **kwargs\n            )\n\n        return _make_request()\n    else:\n        return client.responses(model=model_name, input=input, temperature=temperature, max_tokens=max_tokens, **kwargs)\n</code></pre>"},{"location":"api/responses/#ullm.aresponses","title":"<code>aresponses(model, input, temperature=None, max_tokens=None, num_retries=3, retry_strategy='exponential_backoff_retry', cache=None, api_key=None, api_base=None, timeout=600.0, **kwargs)</code>  <code>async</code>","text":"<p>Make an async request using OpenAI's Responses API format.</p> <p>Compatible with litellm.aresponses() API.</p> <p>Returns:</p> Type Description <code>ModelResponse</code> <p>ModelResponse</p> Source code in <code>ullm/main.py</code> <pre><code>async def aresponses(\n    model: str,\n    input: list[Dict[str, Any]],\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    num_retries: int = 3,\n    retry_strategy: str = \"exponential_backoff_retry\",\n    cache: Optional[Dict[str, Any]] = None,\n    api_key: Optional[str] = None,\n    api_base: Optional[str] = None,\n    timeout: float = 600.0,\n    **kwargs: Any,\n) -&gt; ModelResponse:\n    \"\"\"\n    Make an async request using OpenAI's Responses API format.\n\n    Compatible with litellm.aresponses() API.\n\n    Args:\n        Same as responses()\n\n    Returns:\n        ModelResponse\n    \"\"\"\n    provider, model_name = parse_model_name(model)\n\n    client = _get_client(\n        provider,\n        api_key=api_key,\n        api_base=api_base,\n        timeout=timeout,\n        **kwargs,\n    )\n\n    # Only OpenAI client has aresponses method\n    if not hasattr(client, \"aresponses\"):\n        # Fall back to converting to messages format\n        messages = []\n        for item in input:\n            role = item.get(\"role\", \"user\")\n            content_blocks = item.get(\"content\", [])\n\n            if isinstance(content_blocks, list):\n                content = \" \".join(\n                    block.get(\"text\", \"\") if isinstance(block, dict) else str(block) for block in content_blocks\n                )\n            else:\n                content = str(content_blocks)\n\n            messages.append({\"role\": role, \"content\": content})\n\n        return await acompletion(\n            model=model,\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            num_retries=num_retries,\n            api_key=api_key,\n            api_base=api_base,\n            timeout=timeout,\n            **kwargs,\n        )\n\n    # Use native aresponses method\n    if num_retries &gt; 0:\n\n        @_create_retry_decorator(num_retries)\n        async def _make_request():\n            return await client.aresponses(\n                model=model_name, input=input, temperature=temperature, max_tokens=max_tokens, **kwargs\n            )\n\n        return await _make_request()\n    else:\n        return await client.aresponses(\n            model=model_name, input=input, temperature=temperature, max_tokens=max_tokens, **kwargs\n        )\n</code></pre>"},{"location":"api/types/","title":"Types","text":"<p>(Full API reference coming soon)</p>"},{"location":"api/types/#ullm.types","title":"<code>types</code>","text":"<p>Response types for ullm - compatible with litellm types.</p>"},{"location":"api/types/#ullm.types.FunctionCall","title":"<code>FunctionCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Function call in a message.</p>"},{"location":"api/types/#ullm.types.ToolCall","title":"<code>ToolCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tool call in a message.</p>"},{"location":"api/types/#ullm.types.Message","title":"<code>Message</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Message in a completion response.</p>"},{"location":"api/types/#ullm.types.Usage","title":"<code>Usage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Token usage information.</p>"},{"location":"api/types/#ullm.types.Choice","title":"<code>Choice</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A choice in a completion response.</p>"},{"location":"api/types/#ullm.types.ModelResponse","title":"<code>ModelResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Standard completion response - compatible with litellm.ModelResponse.</p>"},{"location":"api/types/#ullm.types.Delta","title":"<code>Delta</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Delta in a streaming chunk.</p>"},{"location":"api/types/#ullm.types.StreamChoice","title":"<code>StreamChoice</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A choice in a streaming response.</p>"},{"location":"api/types/#ullm.types.StreamChunk","title":"<code>StreamChunk</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Streaming chunk - compatible with litellm streaming.</p>"},{"location":"api/types/#ullm.types.Function","title":"<code>Function</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Function definition for tool calling.</p>"},{"location":"api/types/#ullm.types.Tool","title":"<code>Tool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tool definition.</p>"},{"location":"api/types/#ullm.types.ResponseFormatType","title":"<code>ResponseFormatType</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response format specification.</p>"},{"location":"architecture/decisions/","title":"Architecture Decision Records","text":"<p>This document records the key architectural decisions made in ullm's design.</p>"},{"location":"architecture/decisions/#overview","title":"Overview","text":"<p>ullm was created to provide a lightweight alternative to litellm specifically for DSPy users who need minimal overhead while maintaining API compatibility.</p>"},{"location":"architecture/decisions/#the-problem","title":"The Problem","text":"<p>litellm is excellent but has significant overhead:</p> <ul> <li>~200MB memory footprint</li> <li>~1.2s import time</li> <li>50+ dependencies</li> <li>Loads all 100+ provider implementations upfront</li> </ul>"},{"location":"architecture/decisions/#the-solution","title":"The Solution","text":"<p>ullm (\u03bcLLM) provides:</p> <ul> <li>Only 4 core providers (OpenAI, Anthropic, Groq, AWS Bedrock)</li> <li>API compatibility with litellm</li> <li>Modern, efficient tooling</li> <li>Minimal dependencies (~2MB footprint, 3 dependencies)</li> </ul>"},{"location":"architecture/decisions/#adr-001-use-httpx-for-http","title":"ADR-001: Use httpx for HTTP","text":"<p>Decision: Use httpx as the single HTTP library for both sync and async operations.</p> <p>Rationale:</p> <ul> <li>Single library reduces dependencies</li> <li>Modern, async-first design</li> <li>Supports both sync and async with same API</li> <li>Smaller footprint than requests + aiohttp</li> <li>Excellent streaming support</li> </ul> <p>Status: \u2705 Implemented</p>"},{"location":"architecture/decisions/#adr-002-use-tenacity-for-retry-logic","title":"ADR-002: Use tenacity for retry logic","text":"<p>Decision: Use tenacity library for exponential backoff retry logic.</p> <p>Rationale:</p> <ul> <li>Well-tested, battle-hardened</li> <li>Configurable retry strategies</li> <li>Only ~200KB overhead</li> <li>Handles edge cases (jitter, max delay, etc.)</li> </ul> <p>Status: \u2705 Implemented</p>"},{"location":"architecture/decisions/#adr-003-no-built-in-caching","title":"ADR-003: No built-in caching","text":"<p>Decision: Do not implement caching in ullm itself.</p> <p>Rationale:</p> <ul> <li>DSPy already has its own caching</li> <li>Caching adds complexity and state</li> <li>Would increase memory footprint</li> <li>Users can add their own caching layer</li> <li>Accept <code>cache</code> parameter for API compatibility but ignore it</li> </ul> <p>Status: \u2705 Implemented (passthrough only)</p>"},{"location":"architecture/decisions/#adr-004-pydantic-v2-for-validation","title":"ADR-004: Pydantic v2 for validation","text":"<p>Decision: Use Pydantic v2 for response types and validation.</p> <p>Rationale:</p> <ul> <li>Modern, fast (Rust core, 5-50x faster than v1)</li> <li>Excellent JSON schema generation</li> <li>Type-safe models</li> <li>Supports structured output validation</li> <li>~1MB dependency</li> </ul> <p>Status: \u2705 Implemented</p>"},{"location":"architecture/decisions/#adr-005-provider-specific-client-classes","title":"ADR-005: Provider-specific client classes","text":"<p>Decision: Separate client class for each provider.</p> <p>Rationale:</p> <ul> <li>Clean separation of concerns</li> <li>Each provider has unique API quirks</li> <li>Easy to add/remove providers</li> <li>Groq can inherit from OpenAI (API-compatible)</li> <li>Testable in isolation</li> </ul> <p>Structure: <pre><code>BaseClient (ABC)\n\u251c\u2500\u2500 OpenAIClient\n\u251c\u2500\u2500 AnthropicClient\n\u251c\u2500\u2500 GroqClient (inherits OpenAIClient)\n\u2514\u2500\u2500 BedrockClient\n</code></pre></p> <p>Status: \u2705 Implemented</p>"},{"location":"architecture/decisions/#adr-006-client-registry-pattern","title":"ADR-006: Client Registry Pattern","text":"<p>Decision: Use decorator-based registry for client registration instead of if/elif chains.</p> <p>Rationale:</p> <ul> <li>Cleaner, more maintainable code</li> <li>Each client self-registers with <code>@register_client(provider)</code></li> <li>No need to modify main.py when adding providers</li> <li>Better separation of concerns</li> <li>No circular dependencies</li> </ul> <p>Implementation: <pre><code>@register_client(\"openai\")\nclass OpenAIClient(BaseClient):\n    ...\n</code></pre></p> <p>Status: \u2705 Implemented</p>"},{"location":"architecture/decisions/#adr-007-minimal-python-38-support","title":"ADR-007: Minimal Python 3.8+ support","text":"<p>Decision: Target Python 3.8+ (same as litellm).</p> <p>Rationale:</p> <ul> <li>Good ecosystem support</li> <li>DSPy targets similar range</li> <li>Modern syntax (type hints, async/await)</li> <li>Still widely used despite 3.8 EOL</li> </ul> <p>Status: \u2705 Implemented</p>"},{"location":"architecture/decisions/#adr-008-streaming-returns-iterators","title":"ADR-008: Streaming returns iterators","text":"<p>Decision: Streaming returns bare Iterator/AsyncIterator, not custom wrapper classes.</p> <p>Rationale:</p> <ul> <li>Simpler API</li> <li>More Pythonic</li> <li>litellm's streaming wrappers add complexity</li> <li>Easy iteration: <code>for chunk in response</code></li> </ul> <p>Status: \u2705 Implemented</p>"},{"location":"architecture/decisions/#adr-009-openai-format-for-tool-calling","title":"ADR-009: OpenAI format for tool calling","text":"<p>Decision: Always accept and return tools in OpenAI format, convert internally per provider.</p> <p>Rationale:</p> <ul> <li>OpenAI format is most common</li> <li>Easier for users (one format to learn)</li> <li>Internal conversion keeps API clean</li> <li>All providers have similar concepts</li> </ul> <p>Status: \u2705 Implemented</p>"},{"location":"architecture/decisions/#adr-010-structured-output-via-pydantic","title":"ADR-010: Structured output via Pydantic","text":"<p>Decision: Accept <code>response_format</code> as either <code>{\"type\": \"json_object\"}</code> or a Pydantic model class.</p> <p>Rationale:</p> <ul> <li>Pydantic models provide schema + validation</li> <li>Automatic JSON schema generation</li> <li>Type-safe parsing</li> <li>Better than raw JSON strings</li> </ul> <p>Example: <pre><code>class Person(BaseModel):\n    name: str\n    age: int\n\nresponse = ullm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[...],\n    response_format=Person\n)\n</code></pre></p> <p>Status: \u2705 Implemented</p>"},{"location":"architecture/decisions/#adr-011-custom-exception-hierarchy","title":"ADR-011: Custom exception hierarchy","text":"<p>Decision: Define exception hierarchy matching litellm's exceptions.</p> <p>Rationale:</p> <ul> <li>Predictable error handling for users</li> <li>Easy to catch specific error types</li> <li>Compatible with existing DSPy error handling</li> <li>Include model and provider info</li> </ul> <p>Hierarchy: <pre><code>UllmException (base)\n\u251c\u2500\u2500 AuthenticationError (401)\n\u251c\u2500\u2500 BadRequestError (400)\n\u251c\u2500\u2500 RateLimitError (429)\n\u251c\u2500\u2500 Timeout (504)\n\u2514\u2500\u2500 APIError (500+)\n</code></pre></p> <p>Status: \u2705 Implemented</p>"},{"location":"architecture/decisions/#adr-012-use-uv-for-development","title":"ADR-012: Use uv for development","text":"<p>Decision: Use <code>uv</code> for package management, not pip or poetry.</p> <p>Rationale:</p> <ul> <li>Fastest Python package installer (10-100x faster)</li> <li>Modern tool by Astral (makers of ruff)</li> <li>Simple, no lock files for library</li> <li>Growing ecosystem support</li> </ul> <p>Status: \u2705 Implemented</p>"},{"location":"architecture/decisions/#adr-013-use-ruff-for-lintingformatting","title":"ADR-013: Use ruff for linting/formatting","text":"<p>Decision: Use ruff as single tool for linting and formatting.</p> <p>Rationale:</p> <ul> <li>10-100x faster than black + flake8</li> <li>Single tool, one config</li> <li>Rust-based, well-maintained</li> <li>Includes isort, pyupgrade, etc.</li> </ul> <p>Status: \u2705 Implemented</p>"},{"location":"architecture/decisions/#adr-014-relaxed-mypy-configuration","title":"ADR-014: Relaxed mypy configuration","text":"<p>Decision: Use relaxed mypy settings, not strict mode.</p> <p>Rationale:</p> <ul> <li>Some type errors from dynamic provider dispatch</li> <li>boto3 has poor type stubs</li> <li>Async return types complex with Union[ModelResponse, Iterator]</li> <li>Pragmatic approach for MVP</li> </ul> <p>Status: \u2705 Implemented</p>"},{"location":"architecture/decisions/#adr-015-boto3-as-optional-dependency","title":"ADR-015: boto3 as optional dependency","text":"<p>Decision: Make boto3 an optional dependency via extras: <code>ullm[aws]</code>.</p> <p>Rationale:</p> <ul> <li>boto3 is large (~20MB)</li> <li>Not all users need AWS Bedrock</li> <li>Keeps base install small</li> <li>Fails gracefully with clear error message</li> </ul> <p>Installation: <pre><code>pip install ullm        # No boto3\npip install ullm[aws]   # With boto3\n</code></pre></p> <p>Status: \u2705 Implemented</p>"},{"location":"architecture/decisions/#design-principles","title":"Design Principles","text":"<ol> <li>Lightweight First: Every line of code and dependency must justify its existence</li> <li>Compatibility Second: Stay compatible with litellm where practical, not at cost of bloat</li> <li>Modern Tooling: Use the best tools of 2025 (uv, ruff, httpx)</li> <li>Pragmatic Over Perfect: Ship working code, iterate based on feedback</li> <li>Clear Over Clever: Readable code beats clever abstractions</li> </ol>"},{"location":"architecture/decisions/#known-trade-offs","title":"Known Trade-offs","text":""},{"location":"architecture/decisions/#1-limited-provider-support","title":"1. Limited Provider Support","text":"<p>Trade-off: Only 4 providers vs 100+ in litellm Rationale: Dramatically smaller footprint, easier maintenance. Most DSPy users only need 1-2.</p>"},{"location":"architecture/decisions/#2-no-built-in-caching","title":"2. No Built-in Caching","text":"<p>Trade-off: No response caching built-in Rationale: DSPy has its own caching. Keeps ullm simpler and smaller.</p>"},{"location":"architecture/decisions/#3-bedrock-async-uses-thread-pool","title":"3. Bedrock Async Uses Thread Pool","text":"<p>Trade-off: boto3 is synchronous, so async uses thread pool Rationale: Functional but not truly async. Could use aioboto3 in future.</p>"},{"location":"architecture/decisions/#4-no-legacy-text-completion-api","title":"4. No Legacy Text Completion API","text":"<p>Trade-off: No <code>text_completion()</code> function Rationale: 95% of use cases use chat completion. Can add if needed.</p>"},{"location":"architecture/decisions/#future-considerations","title":"Future Considerations","text":"<p>Potential additions (priority order):</p> <ol> <li>Embeddings API (Medium) - DSPy uses embeddings for retrieval</li> <li>Image Input Support (Medium) - Vision is becoming more common</li> <li>Additional Providers (Low) - Only if users request them</li> <li>Cost Tracking (Low) - Useful for monitoring</li> <li>Lightweight Caching (Low) - Optional in-memory LRU cache</li> </ol>"},{"location":"architecture/decisions/#things-to-not-add","title":"Things to NOT Add","text":"<ul> <li>\u274c Proxy server mode</li> <li>\u274c Spend analytics dashboard</li> <li>\u274c Fine-tuning API</li> <li>\u274c Prompt templates (DSPy handles this)</li> <li>\u274c Evaluation frameworks (DSPy handles this)</li> </ul>"},{"location":"architecture/providers/","title":"Provider System","text":"<p>How ullm resolves and manages providers.</p> <p>(More details coming soon)</p>"},{"location":"architecture/registry/","title":"Client Registry","text":"<p>The client registry pattern allows clients to self-register.</p> <pre><code>@register_client(\"openai\")\nclass OpenAIClient(BaseClient):\n    ...\n</code></pre> <p>(More details coming soon)</p>"},{"location":"development/contributing/","title":"Contributing","text":"<p>See CONTRIBUTING.md</p>"},{"location":"development/releases/","title":"Release Process","text":"<p>This guide explains how to release new versions of ullm.</p>"},{"location":"development/releases/#overview","title":"Overview","text":"<p>ullm uses GitHub Releases and GitHub Actions for automated publishing to PyPI. The process is designed to be simple and safe.</p>"},{"location":"development/releases/#prerequisites","title":"Prerequisites","text":""},{"location":"development/releases/#one-time-setup","title":"One-Time Setup","text":"<ol> <li>PyPI Account</li> <li>Create account at https://pypi.org/account/register/</li> <li> <p>Verify your email</p> </li> <li> <p>PyPI API Token</p> </li> <li>Go to https://pypi.org/manage/account/token/</li> <li>Click \"Add API token\"</li> <li>Name: <code>github-actions-ullm</code></li> <li>Scope: \"Entire account\" (or specific to ullm after first upload)</li> <li> <p>Copy the token (starts with <code>pypi-</code>)</p> </li> <li> <p>Add Secret to GitHub</p> </li> <li>Go to https://github.com/silvestrid/ullm/settings/secrets/actions</li> <li>Click \"New repository secret\"</li> <li>Name: <code>PYPI_API_TOKEN</code></li> <li>Value: Paste your PyPI token</li> <li>Click \"Add secret\"</li> </ol>"},{"location":"development/releases/#release-checklist","title":"Release Checklist","text":""},{"location":"development/releases/#1-pre-release-checks","title":"1. Pre-Release Checks","text":"<ul> <li>[ ] All tests passing on main branch</li> <li>[ ] Documentation is up to date</li> <li>[ ] CHANGELOG.md updated with new version</li> <li>[ ] No uncommitted changes</li> </ul> <pre><code># Verify tests pass\njust test\n\n# Verify linting passes\njust lint\n\n# Verify you're on main and up to date\ngit checkout main\ngit pull origin main\ngit status\n</code></pre>"},{"location":"development/releases/#2-update-version","title":"2. Update Version","text":"<p>Update the version in <code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"ullm\"\nversion = \"0.2.0\"  # \u2190 Update this\n</code></pre>"},{"location":"development/releases/#3-update-changelog","title":"3. Update Changelog","text":"<p>Add release notes to <code>CHANGELOG.md</code>:</p> <pre><code>## [0.2.0] - 2025-01-15\n\n### Added\n- New feature X\n- Support for Y\n\n### Changed\n- Improved Z performance\n\n### Fixed\n- Bug in A\n- Issue with B\n</code></pre>"},{"location":"development/releases/#4-commit-and-tag","title":"4. Commit and Tag","text":"<pre><code># Commit version bump\ngit add pyproject.toml CHANGELOG.md\ngit commit -m \"chore: bump version to 0.2.0\"\n\n# Create annotated tag\ngit tag -a v0.2.0 -m \"Release v0.2.0\"\n\n# Push commit and tags\ngit push origin main\ngit push origin v0.2.0\n</code></pre>"},{"location":"development/releases/#5-create-github-release","title":"5. Create GitHub Release","text":"<ol> <li>Go to https://github.com/silvestrid/ullm/releases/new</li> <li>Select tag: <code>v0.2.0</code></li> <li>Release title: <code>v0.2.0</code></li> <li>Description: Copy from CHANGELOG.md</li> <li>Check \"Set as latest release\"</li> <li>Click \"Publish release\"</li> </ol> <p>That's it! GitHub Actions will automatically: - Build the package - Publish to PyPI - Update documentation (if configured)</p>"},{"location":"development/releases/#6-verify-release","title":"6. Verify Release","text":"<p>Wait a few minutes, then verify:</p> <ol> <li>Check GitHub Actions</li> <li>Go to https://github.com/silvestrid/ullm/actions</li> <li> <p>Verify \"Publish to PyPI\" workflow succeeded</p> </li> <li> <p>Check PyPI</p> </li> <li>Go to https://pypi.org/project/ullm/</li> <li> <p>Verify new version is live</p> </li> <li> <p>Test Installation <pre><code>pip install ullm==0.2.0\npython -c \"import ullm; print(ullm.__version__)\"\n</code></pre></p> </li> </ol>"},{"location":"development/releases/#version-numbering","title":"Version Numbering","text":"<p>ullm follows Semantic Versioning:</p>"},{"location":"development/releases/#format-majorminorpatch","title":"Format: <code>MAJOR.MINOR.PATCH</code>","text":"<ul> <li>MAJOR (1.0.0): Breaking changes</li> <li>MINOR (0.1.0): New features, backwards compatible</li> <li>PATCH (0.0.1): Bug fixes, backwards compatible</li> </ul>"},{"location":"development/releases/#examples","title":"Examples","text":"<pre><code>0.1.0 \u2192 0.1.1   # Bug fix\n0.1.1 \u2192 0.2.0   # New feature\n0.2.0 \u2192 1.0.0   # Breaking change\n</code></pre>"},{"location":"development/releases/#when-to-bump","title":"When to Bump","text":"<p>Patch (0.0.X): Bug fixes only - Fixing a bug - Documentation updates - Performance improvements (no API changes)</p> <p>Minor (0.X.0): New features, backwards compatible - Adding new provider - Adding new optional parameters - Adding new exception types - Deprecating features (with warnings)</p> <p>Major (X.0.0): Breaking changes - Removing public API - Changing function signatures - Removing providers - Changing exception hierarchy</p>"},{"location":"development/releases/#hotfix-releases","title":"Hotfix Releases","text":"<p>For urgent bug fixes:</p> <pre><code># Create hotfix branch from latest release tag\ngit checkout -b hotfix/0.1.1 v0.1.0\n\n# Make fixes\ngit add .\ngit commit -m \"fix: critical bug\"\n\n# Update version to 0.1.1\n# Update CHANGELOG.md\n\n# Commit, tag, and push\ngit commit -am \"chore: bump version to 0.1.1\"\ngit tag -a v0.1.1 -m \"Hotfix v0.1.1\"\ngit push origin hotfix/0.1.1\ngit push origin v0.1.1\n\n# Create GitHub Release\n# Merge back to main\ngit checkout main\ngit merge hotfix/0.1.1\ngit push origin main\n</code></pre>"},{"location":"development/releases/#rolling-back-a-release","title":"Rolling Back a Release","text":"<p>If a release has critical issues:</p>"},{"location":"development/releases/#option-1-yank-from-pypi","title":"Option 1: Yank from PyPI","text":"<pre><code># Install twine\npip install twine\n\n# Yank the version (makes it unavailable but keeps record)\ntwine yank ullm==0.2.0 -r pypi\n</code></pre>"},{"location":"development/releases/#option-2-release-a-fixed-version","title":"Option 2: Release a Fixed Version","text":"<pre><code># Fix the issue\n# Bump to 0.2.1\n# Follow normal release process\n</code></pre> <p>Don't Delete Releases</p> <p>Don't delete releases from PyPI or GitHub. Use yank or release a fix instead.</p>"},{"location":"development/releases/#pre-releases","title":"Pre-releases","text":"<p>For beta/RC releases:</p> <pre><code># pyproject.toml\nversion = \"0.2.0b1\"  # or 0.2.0rc1\n</code></pre> <pre><code>git tag -a v0.2.0b1 -m \"Beta release v0.2.0b1\"\ngit push origin v0.2.0b1\n</code></pre> <p>Mark as \"pre-release\" when creating GitHub Release.</p>"},{"location":"development/releases/#release-automation","title":"Release Automation","text":"<p>The <code>.github/workflows/publish.yml</code> workflow:</p> <pre><code>name: Publish to PyPI\n\non:\n  release:\n    types: [published]\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n    - name: Install uv\n      uses: astral-sh/setup-uv@v4\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: '3.11'\n    - name: Build package\n      run: uv build\n    - name: Publish to PyPI\n      uses: pypa/gh-action-pypi-publish@release/v1\n      with:\n        password: ${{ secrets.PYPI_API_TOKEN }}\n</code></pre>"},{"location":"development/releases/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/releases/#build-fails","title":"Build Fails","text":"<p>Issue: Package build fails</p> <p>Solution: <pre><code># Build locally first\nuv build\n\n# Check for issues\nls dist/\n</code></pre></p>"},{"location":"development/releases/#pypi-upload-fails","title":"PyPI Upload Fails","text":"<p>Issue: Authentication error</p> <p>Solution: - Verify <code>PYPI_API_TOKEN</code> secret is set correctly - Check token hasn't expired - Ensure token has correct permissions</p> <p>Issue: Version already exists</p> <p>Solution: - PyPI doesn't allow re-uploading same version - Bump version and release again - If testing, use TestPyPI first</p>"},{"location":"development/releases/#wrong-version-number","title":"Wrong Version Number","text":"<p>Issue: Released wrong version number</p> <p>Solution: - Can't change PyPI version after upload - Release a new version with correct number - Update documentation to skip wrong version</p>"},{"location":"development/releases/#best-practices","title":"Best Practices","text":"<ol> <li>\u2705 Test Before Release</li> <li>Run full test suite</li> <li>Test installation in clean environment</li> <li> <p>Try examples</p> </li> <li> <p>\u2705 Update Documentation</p> </li> <li>Update CHANGELOG.md</li> <li>Update version in docs</li> <li> <p>Add migration notes if breaking</p> </li> <li> <p>\u2705 Use Annotated Tags <pre><code>git tag -a v0.2.0 -m \"Release v0.2.0\"  # Good\ngit tag v0.2.0                          # Bad\n</code></pre></p> </li> <li> <p>\u2705 Write Good Release Notes</p> </li> <li>Explain what changed and why</li> <li>Include migration instructions</li> <li> <p>Credit contributors</p> </li> <li> <p>\u2705 Test the Release</p> </li> <li>Install from PyPI</li> <li>Run smoke tests</li> <li>Check documentation links</li> </ol>"},{"location":"development/releases/#release-schedule","title":"Release Schedule","text":"<p>ullm doesn't have a fixed release schedule. Releases happen when:</p> <ul> <li>Patch: As needed for bug fixes</li> <li>Minor: When new features are ready</li> <li>Major: Only when breaking changes are necessary</li> </ul> <p>Typical cadence: 1-2 months between minor releases.</p>"},{"location":"development/releases/#communication","title":"Communication","text":"<p>After releasing:</p> <ol> <li>Announce on GitHub</li> <li> <p>Release notes automatically notify watchers</p> </li> <li> <p>Update Documentation</p> </li> <li> <p>Documentation site updates automatically</p> </li> <li> <p>Community</p> </li> <li>Share on relevant channels if major release</li> <li>Respond to feedback and issues</li> </ol>"},{"location":"development/releases/#questions","title":"Questions?","text":"<ul> <li>See Contributing Guide for development workflow</li> <li>Open an issue for release process questions</li> <li>Check GitHub Actions logs for build/publish issues</li> </ul>"},{"location":"development/setup/","title":"Development Setup","text":"<pre><code>git clone https://github.com/silvestrid/ullm.git\ncd ullm\njust dev\nsource .venv/bin/activate\njust install\n</code></pre> <p>(More details coming soon)</p>"},{"location":"development/testing/","title":"Testing","text":"<pre><code>just test\njust test-cov\n</code></pre> <p>(More details coming soon)</p>"},{"location":"getting-started/basic-usage/","title":"Basic Usage","text":"<p>Learn the fundamentals of using ullm.</p>"},{"location":"getting-started/basic-usage/#simple-completion","title":"Simple Completion","text":"<p>The most basic operation is a completion request:</p> <pre><code>import ullm\n\nresponse = ullm.completion(\n    model=\"openai/gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"getting-started/basic-usage/#message-format","title":"Message Format","text":"<p>Messages follow the OpenAI chat format:</p> <pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is Python?\"},\n    {\"role\": \"assistant\", \"content\": \"Python is a programming language.\"},\n    {\"role\": \"user\", \"content\": \"Tell me more.\"}\n]\n\nresponse = ullm.completion(model=\"gpt-4o-mini\", messages=messages)\n</code></pre>"},{"location":"getting-started/basic-usage/#roles","title":"Roles","text":"<ul> <li>system: Sets behavior and context</li> <li>user: User messages</li> <li>assistant: Assistant responses (for conversation history)</li> </ul>"},{"location":"getting-started/basic-usage/#model-names","title":"Model Names","text":"<p>ullm supports multiple formats:</p> <pre><code># With provider prefix (recommended)\nmodel=\"openai/gpt-4o-mini\"\nmodel=\"anthropic/claude-3-5-sonnet-20241022\"\nmodel=\"groq/llama-3.1-70b-versatile\"\nmodel=\"bedrock/anthropic.claude-3-sonnet\"\n\n# Auto-detect provider (for well-known models)\nmodel=\"gpt-4o-mini\"              # Detects OpenAI\nmodel=\"claude-3-5-sonnet-20241022\"  # Detects Anthropic\n</code></pre>"},{"location":"getting-started/basic-usage/#common-parameters","title":"Common Parameters","text":""},{"location":"getting-started/basic-usage/#temperature","title":"Temperature","text":"<p>Controls randomness (0-2):</p> <pre><code># More focused and deterministic\nresponse = ullm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[...],\n    temperature=0.0\n)\n\n# More creative and random\nresponse = ullm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[...],\n    temperature=1.0\n)\n</code></pre>"},{"location":"getting-started/basic-usage/#max-tokens","title":"Max Tokens","text":"<p>Limit response length:</p> <pre><code>response = ullm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[...],\n    max_tokens=100  # Limit to 100 tokens\n)\n</code></pre>"},{"location":"getting-started/basic-usage/#timeout","title":"Timeout","text":"<p>Set request timeout:</p> <pre><code>response = ullm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[...],\n    timeout=30.0  # 30 seconds\n)\n</code></pre>"},{"location":"getting-started/basic-usage/#response-format","title":"Response Format","text":"<p>The response is a <code>ModelResponse</code> object:</p> <pre><code>response = ullm.completion(model=\"gpt-4o-mini\", messages=[...])\n\n# Access the message content\ncontent = response.choices[0].message.content\n\n# Access usage information\nprint(f\"Prompt tokens: {response.usage.prompt_tokens}\")\nprint(f\"Completion tokens: {response.usage.completion_tokens}\")\nprint(f\"Total tokens: {response.usage.total_tokens}\")\n\n# Access model and other metadata\nprint(f\"Model: {response.model}\")\nprint(f\"Finish reason: {response.choices[0].finish_reason}\")\n</code></pre>"},{"location":"getting-started/basic-usage/#api-keys","title":"API Keys","text":""},{"location":"getting-started/basic-usage/#from-environment","title":"From Environment","text":"<pre><code>export OPENAI_API_KEY=sk-...\nexport ANTHROPIC_API_KEY=sk-ant-...\nexport GROQ_API_KEY=gsk_...\n</code></pre> <pre><code># Keys automatically loaded from environment\nresponse = ullm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[...]\n)\n</code></pre>"},{"location":"getting-started/basic-usage/#direct-pass","title":"Direct Pass","text":"<pre><code>response = ullm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[...],\n    api_key=\"sk-...\"\n)\n</code></pre>"},{"location":"getting-started/basic-usage/#custom-api-base","title":"Custom API Base","text":"<pre><code>response = ullm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[...],\n    api_base=\"https://custom.openai.endpoint\"\n)\n</code></pre>"},{"location":"getting-started/basic-usage/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    response = ullm.completion(\n        model=\"gpt-4o-mini\",\n        messages=[...]\n    )\nexcept ullm.AuthenticationError as e:\n    print(f\"Invalid API key: {e}\")\nexcept ullm.RateLimitError as e:\n    print(f\"Rate limited: {e}\")\nexcept ullm.BadRequestError as e:\n    print(f\"Bad request: {e}\")\nexcept ullm.Timeout as e:\n    print(f\"Timeout: {e}\")\nexcept ullm.APIError as e:\n    print(f\"API error: {e}\")\n</code></pre> <p>See Error Handling Guide for details.</p>"},{"location":"getting-started/basic-usage/#retry-logic","title":"Retry Logic","text":"<p>ullm automatically retries on rate limits and timeouts:</p> <pre><code>response = ullm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[...],\n    num_retries=3  # Retry up to 3 times (default)\n)\n\n# Disable retries\nresponse = ullm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[...],\n    num_retries=0\n)\n</code></pre>"},{"location":"getting-started/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Streaming - Stream responses</li> <li>Tool Calling - Use functions/tools</li> <li>Structured Output - Get JSON responses</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>pip or uv</li> </ul>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>Install ullm using pip:</p> <pre><code>pip install ullm\n</code></pre> <p>Or using uv (faster):</p> <pre><code>uv pip install ullm\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#aws-bedrock-support","title":"AWS Bedrock Support","text":"<p>If you need AWS Bedrock support, install with the <code>aws</code> extra:</p> <pre><code>pip install ullm[aws]\n</code></pre> <p>This installs <code>boto3</code> which is required for AWS Bedrock integration.</p>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development, install with dev dependencies:</p> <pre><code>pip install ullm[dev]\n</code></pre> <p>This includes: - pytest (testing) - ruff (linting and formatting) - mypy (type checking) - pytest-asyncio (async testing) - pytest-cov (coverage)</p>"},{"location":"getting-started/installation/#documentation","title":"Documentation","text":"<p>To build the documentation locally:</p> <pre><code>pip install ullm[docs]\n</code></pre>"},{"location":"getting-started/installation/#all-optional-dependencies","title":"All Optional Dependencies","text":"<p>To install everything:</p> <pre><code>pip install ullm[aws,dev,docs]\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>Clone the repository and install in development mode:</p> <pre><code>git clone https://github.com/silvestrid/ullm.git\ncd ullm\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Verify your installation:</p> <pre><code>import ullm\n\nprint(f\"ullm version: {ullm.__version__}\")\nprint(f\"Available functions: {dir(ullm)}\")\n</code></pre>"},{"location":"getting-started/installation/#api-keys","title":"API Keys","text":"<p>ullm reads API keys from environment variables:</p> <pre><code># OpenAI\nexport OPENAI_API_KEY=sk-...\n\n# Anthropic\nexport ANTHROPIC_API_KEY=sk-ant-...\n\n# Groq\nexport GROQ_API_KEY=gsk_...\n\n# AWS Bedrock (uses standard AWS credentials)\nexport AWS_ACCESS_KEY_ID=...\nexport AWS_SECRET_ACCESS_KEY=...\nexport AWS_REGION_NAME=us-east-1\n</code></pre> <p>Or pass them directly:</p> <pre><code>import ullm\n\nresponse = ullm.completion(\n    model=\"openai/gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n    api_key=\"sk-...\"\n)\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get started in 5 minutes</li> <li>Basic Usage - Learn the fundamentals</li> <li>User Guide - Explore all features</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>Get started with ullm in 5 minutes!</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<pre><code>pip install ullm\n</code></pre>"},{"location":"getting-started/quickstart/#set-api-key","title":"Set API Key","text":"<pre><code>export OPENAI_API_KEY=sk-...\n</code></pre>"},{"location":"getting-started/quickstart/#your-first-request","title":"Your First Request","text":"<pre><code>import ullm\n\nresponse = ullm.completion(\n    model=\"openai/gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <p>That's it! You've made your first LLM request with ullm.</p>"},{"location":"getting-started/quickstart/#try-different-providers","title":"Try Different Providers","text":"OpenAIAnthropicGroqAWS Bedrock <pre><code>response = ullm.completion(\n    model=\"openai/gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre> <pre><code># Set key: export ANTHROPIC_API_KEY=sk-ant-...\nresponse = ullm.completion(\n    model=\"anthropic/claude-3-5-sonnet-20241022\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre> <pre><code># Set key: export GROQ_API_KEY=gsk_...\nresponse = ullm.completion(\n    model=\"groq/llama-3.1-70b-versatile\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre> <pre><code># Requires: pip install ullm[aws]\n# AWS credentials from environment\nresponse = ullm.completion(\n    model=\"bedrock/anthropic.claude-3-sonnet\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre>"},{"location":"getting-started/quickstart/#add-streaming","title":"Add Streaming","text":"<pre><code>for chunk in ullm.completion(\n    model=\"openai/gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a poem\"}],\n    stream=True\n):\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n</code></pre>"},{"location":"getting-started/quickstart/#use-async","title":"Use Async","text":"<pre><code>import asyncio\n\nasync def main():\n    response = await ullm.acompletion(\n        model=\"openai/gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\n    print(response.choices[0].message.content)\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#control-parameters","title":"Control Parameters","text":"<pre><code>response = ullm.completion(\n    model=\"openai/gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Be creative!\"}],\n    temperature=0.9,      # More random (0-2)\n    max_tokens=500,       # Limit response length\n    num_retries=3,        # Retry on failure\n    timeout=60.0          # Timeout in seconds\n)\n</code></pre>"},{"location":"getting-started/quickstart/#handle-errors","title":"Handle Errors","text":"<pre><code>try:\n    response = ullm.completion(\n        model=\"openai/gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\nexcept ullm.AuthenticationError:\n    print(\"Invalid API key\")\nexcept ullm.RateLimitError:\n    print(\"Rate limit exceeded - will auto-retry\")\nexcept ullm.APIError as e:\n    print(f\"API error: {e}\")\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you're up and running:</p> <ul> <li>Basic Usage - Learn the fundamentals</li> <li>User Guide - Explore all features</li> <li>API Reference - Detailed API docs</li> <li>Examples - See more examples</li> </ul>"},{"location":"getting-started/quickstart/#common-issues","title":"Common Issues","text":"<p>ModuleNotFoundError: No module named 'ullm'</p> <p>Make sure ullm is installed: <code>pip install ullm</code></p> <p>AuthenticationError: Invalid API key</p> <p>Set your API key: <code>export OPENAI_API_KEY=sk-...</code></p> <p>ImportError: boto3 not found (for Bedrock)</p> <p>Install AWS extras: <code>pip install ullm[aws]</code></p> <p>Different results than litellm?</p> <p>ullm is designed as a drop-in replacement but may have slight differences in token counting or streaming chunk formatting.</p>"},{"location":"guide/dspy-integration/","title":"DSPy Integration","text":"<p>Use ullm as a drop-in replacement for litellm in DSPy.</p> <pre><code>import dspy\nimport ullm\n\n# Configure DSPy to use ullm\nlm = dspy.LM(model=\"openai/gpt-4o-mini\")\ndspy.configure(lm=lm)\n</code></pre> <p>(More details coming soon)</p>"},{"location":"guide/error-handling/","title":"Error Handling","text":"<p>Handle errors and configure retry logic.</p> <pre><code>try:\n    response = ullm.completion(model=\"gpt-4o-mini\", messages=[...])\nexcept ullm.AuthenticationError:\n    print(\"Invalid API key\")\nexcept ullm.RateLimitError:\n    print(\"Rate limited\")\nexcept ullm.APIError as e:\n    print(f\"Error: {e}\")\n</code></pre> <p>(More details coming soon)</p>"},{"location":"guide/overview/","title":"User Guide Overview","text":"<p>Welcome to the ullm user guide. This section covers all features in depth.</p>"},{"location":"guide/overview/#topics","title":"Topics","text":"<ul> <li>Providers - Working with different LLM providers</li> <li>Streaming - Stream responses in real-time</li> <li>Tool Calling - Use functions and tools</li> <li>Structured Output - Get validated JSON responses</li> <li>Error Handling - Handle errors and retries</li> <li>DSPy Integration - Use ullm with DSPy</li> </ul>"},{"location":"guide/overview/#quick-links","title":"Quick Links","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>API Reference</li> </ul>"},{"location":"guide/providers/","title":"Working with Providers","text":"<p>ullm supports four LLM providers. Each has its own quirks and capabilities.</p>"},{"location":"guide/providers/#openai","title":"OpenAI","text":"<pre><code>response = ullm.completion(\n    model=\"openai/gpt-4o-mini\",\n    messages=[...]\n)\n</code></pre>"},{"location":"guide/providers/#anthropic","title":"Anthropic","text":"<pre><code>response = ullm.completion(\n    model=\"anthropic/claude-3-5-sonnet-20241022\",\n    messages=[...]\n)\n</code></pre>"},{"location":"guide/providers/#groq","title":"Groq","text":"<pre><code>response = ullm.completion(\n    model=\"groq/llama-3.1-70b-versatile\",\n    messages=[...]\n)\n</code></pre>"},{"location":"guide/providers/#aws-bedrock","title":"AWS Bedrock","text":"<p>Requires <code>pip install ullm[aws]</code></p> <pre><code>response = ullm.completion(\n    model=\"bedrock/anthropic.claude-3-sonnet\",\n    messages=[...]\n)\n</code></pre> <p>(More details coming soon)</p>"},{"location":"guide/streaming/","title":"Streaming","text":"<p>Stream responses token-by-token.</p>"},{"location":"guide/streaming/#basic-streaming","title":"Basic Streaming","text":"<pre><code>for chunk in ullm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[...],\n    stream=True\n):\n    print(chunk.choices[0].delta.content, end=\"\")\n</code></pre>"},{"location":"guide/streaming/#async-streaming","title":"Async Streaming","text":"<pre><code>async for chunk in await ullm.acompletion(\n    model=\"gpt-4o-mini\",\n    messages=[...],\n    stream=True\n):\n    print(chunk.choices[0].delta.content, end=\"\")\n</code></pre> <p>(More details coming soon)</p>"},{"location":"guide/structured-output/","title":"Structured Output","text":"<p>Get validated JSON responses using Pydantic models.</p> <pre><code>from pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\nresponse = ullm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[...],\n    response_format=Person\n)\n</code></pre> <p>(More details coming soon)</p>"},{"location":"guide/tool-calling/","title":"Tool Calling","text":"<p>Use functions and tools with LLMs.</p> <pre><code>tools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get weather\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\"}\n            }\n        }\n    }\n}]\n\nresponse = ullm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[...],\n    tools=tools\n)\n</code></pre> <p>(More details coming soon)</p>"}]}